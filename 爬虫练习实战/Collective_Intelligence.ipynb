{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "#combine the two links\n",
    "from urllib.parse import urljoin\n",
    "import sqlite3\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a list of words to ignore\n",
    "ignorewords = set(['the','of','to','and','a','in','is','it'])\n",
    "\n",
    "class Crawler:\n",
    "    #Initialize the crawler with the name of database\n",
    "    def __init__(self,dbname):\n",
    "        self.con = sqlite3.connect(dbname)\n",
    "    #close the db\n",
    "    def __del__(self):\n",
    "        self.con.close()\n",
    "    #提交\n",
    "    def dbcommit(self):\n",
    "        self.con.commit()\n",
    "    #Auxilliary function for getting an entry id and adding it\n",
    "    #if it's not present\n",
    "    def getentryid(self,table,field,value,createnew=True):\n",
    "        cur=self.con.execute(\"select rowid from %s where %s='%s'\" %(table,field,value))\n",
    "        res=cur.fetchone()\n",
    "        if res==None:\n",
    "            cur=self.con.execute(\"insert into %s (%s) values ('%s')\" %(table,field,value))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]\n",
    "    #Index an individual page\n",
    "    def addtoindex(self,url,soup):\n",
    "        if self.isindexed(url):\n",
    "            return None\n",
    "        print('Indexing'+url)\n",
    "        \n",
    "        # Get the individual words\n",
    "        text=self.gettextonly(soup)\n",
    "        words=self.separatewords(text)\n",
    "        # Get the URL id\n",
    "        #获取url的id 调用getentryid函数，获取urlid 的rowid\n",
    "        #若数据库中无此urlid的信息则insert\n",
    "        urlid=self.getentryid('urllist','url',url)\n",
    "        # Link each word to this url\n",
    "        for i in range(len(words)):\n",
    "            word=words[i]\n",
    "            if word in ignorewords: \n",
    "                continue\n",
    "            #获取word的id 调用getentryid函数，获取wordid 的rowid\n",
    "            #若数据库中无此wordid的信息则insert\n",
    "            wordid=self.getentryid('wordlist','word',word)\n",
    "            #将urlid 与 wordid location(本例中里location即为第几个word)\n",
    "            self.con.execute(\"insert into wordlocation(urlid,wordid,location) values (%d,%d,%d)\" % (urlid,wordid,i))\n",
    "\n",
    "    #Extract the text from an HTML page(no tags)\n",
    "    def gettextonly(self,soup):\n",
    "        v = soup.string\n",
    "        if v==None:\n",
    "            c=soup.contents\n",
    "            resulttext=''\n",
    "            for t in c:\n",
    "                subtext=self.gettextonly(t)\n",
    "                resulttext+=str(subtext)+'\\n'\n",
    "            return resulttext\n",
    "        else:\n",
    "            return v.strip()\n",
    "    #separate the words by any non-whitespace character\n",
    "    def separatewords(self,text):\n",
    "        splitter=re.compile('\\\\W*')\n",
    "        return [s.lower() for s in splitter.split(text) if s!='']\n",
    "    #Return true if this url is already indexed\n",
    "    def isindexed(self,url):\n",
    "        u=self.con.execute(\"select rowid from urllist where url='%s'\"%url).fetchone()\n",
    "        if u!= None:\n",
    "            v=self.con.execute('select * from wordlocation where urlid=%d'%u[0]).fetchone()\n",
    "            if v!=None:\n",
    "                return True\n",
    "        return False\n",
    "    #Add a link between two pages\n",
    "    def addlinkref(self,urlFrom,urlTo,linkText):\n",
    "        pass\n",
    "    def createindextables(self):\n",
    "        self.con.execute('create table urllist(url)')\n",
    "        self.con.execute('create table wordlist(word)')\n",
    "        self.con.execute('create table wordlocation(urlid,wordid,location)')\n",
    "        self.con.execute('create table link(fromid integer,toid integer)')\n",
    "        self.con.execute('create table linkwords(wordid,linkid)')\n",
    "        self.con.execute('create index wordidx on wordlist(word)')\n",
    "        self.con.execute('create index urlidx on urllist(url)')\n",
    "        self.con.execute('create index wordurlidx on wordlocation(wordid)')\n",
    "        self.con.execute('create index urltoidx on link(toid)')\n",
    "        self.con.execute('create index urlfromidx on link(fromid)')\n",
    "        self.dbcommit()\n",
    "    def crawl(self,pages,depth=2):\n",
    "        for i in range(depth):\n",
    "            newpages = set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c = urlopen(page)\n",
    "                except:\n",
    "                    print(\"Could not open %s\"%page)\n",
    "                    continue\n",
    "                soup = BeautifulSoup(c.read(),\"lxml\")\n",
    "                #add the index for the link\n",
    "                self.addtoindex(page,soup)\n",
    "            \n",
    "                links = soup(\"a\")\n",
    "                for link in links:\n",
    "                    if('href' in dict(link.attrs)):\n",
    "                        #combine the base url and the relative url\n",
    "                        url = urljoin(page,link['href'])\n",
    "                        if url.find(\"'\") != -1:\n",
    "                            continue\n",
    "                        url = url.split('#')[0]\n",
    "                        if url[0:4] == \"http\" and not self.isindexed(url):\n",
    "                            newpages.add(url)\n",
    "                        linkText = self.gettextonly(link)\n",
    "                        self.addlinkref(page,url,linkText)\n",
    "                self.dbcommit()\n",
    "            pages = newpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexinghttps://fr.wikipedia.org/wiki/Oise_(d%C3%A9partement)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:63: FutureWarning: split() requires a non-empty pattern match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexinghttps://fr.wikipedia.org/w/index.php?title=Cergy&action=edit&section=46\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Haute-Loire\n",
      "Indexinghttps://fr.wikipedia.org/wiki/R%C3%A9seau_de_bus_Soci%C3%A9t%C3%A9_de_transports_interurbains_du_Val-d%27Oise\n",
      "Indexinghttps://fr.wikipedia.org/wiki/%C3%89cole_maternelle\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Bastia\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Var_(d%C3%A9partement)\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Toulouse\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Gare_de_Cergy-Saint-Christophe\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Brunoy\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Thiais\n",
      "Indexinghttps://fr.wikipedia.org/wiki/D%C3%A9mographie_de_la_France\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Ermont\n",
      "Indexinghttps://fr.wikipedia.org/w/index.php?title=Cergy&veaction=edit&section=46\n",
      "Indexinghttps://fr.wikipedia.org/w/index.php?title=Cergy&action=edit&section=44\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Amiens\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Hautes-Pyr%C3%A9n%C3%A9es\n",
      "Indexinghttps://sl.wikipedia.org/wiki/Cergy\n",
      "Indexinghttps://fr.wikipedia.org/wiki/Rueil-Malmaison\n",
      "Indexinghttps://fr.wikipedia.org/w/index.php?title=Cergy&printable=yes\n",
      "Indexinghttps://vo.wikipedia.org/wiki/Cergy\n",
      "Indexinghttps://fr.wikipedia.org/w/index.php?title=Cergy&action=edit&section=25\n",
      "Indexinghttp://www.cergypontoise.fr/upload/docs/application/pdf/2011-09/scot_dog.pdf\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-98ad2eb2cb18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcrawler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'searchindex.db'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#crawler.createindextables()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpagelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-726012fa0e36>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(self, pages, depth)\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;31m#add the index for the link\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddtoindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-726012fa0e36>\u001b[0m in \u001b[0;36maddtoindex\u001b[1;34m(self, url, soup)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Get the individual words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettextonly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseparatewords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Get the URL id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-726012fa0e36>\u001b[0m in \u001b[0;36mgettextonly\u001b[1;34m(self, soup)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mresulttext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0msubtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettextonly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[0mresulttext\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresulttext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[1;32m<ipython-input-18-726012fa0e36>\u001b[0m in \u001b[0;36mgettextonly\u001b[1;34m(self, soup)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mresulttext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0msubtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettextonly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[0mresulttext\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresulttext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "pagelist = [\"https://fr.wikipedia.org/wiki/Cergy\"]\n",
    "crawler = Crawler('searchindex.db')\n",
    "#crawler.createindextables()\n",
    "crawler.crawl(pagelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher:\n",
    "  def __init__(self,dbname):\n",
    "    self.con=sqlite3.connect(dbname)\n",
    "\n",
    "  def __del__(self):\n",
    "    self.con.close()\n",
    "\n",
    "  def getmatchrows(self,q):\n",
    "    # Strings to build the query\n",
    "    fieldlist='w0.urlid'\n",
    "    tablelist=''  \n",
    "    clauselist=''\n",
    "    wordids=[]\n",
    "\n",
    "    # Split the words by spaces\n",
    "    words=q.split(' ')  \n",
    "    tablenumber=0\n",
    "\n",
    "    for word in words:\n",
    "      # Get the word ID\n",
    "      wordrow=self.con.execute(\n",
    "      \"select rowid from wordlist where word='%s'\" % word).fetchone()\n",
    "      if wordrow!=None:\n",
    "        wordid=wordrow[0]\n",
    "        wordids.append(wordid)\n",
    "        if tablenumber>0:\n",
    "          tablelist +=','\n",
    "          clauselist +=' and '\n",
    "          clauselist += '{0}.urlid={1}.urlid and '.format(tablenumber-1,tablenumber)\n",
    "        fieldlist+='{0}.location'.format(tablenumber)\n",
    "        tablelist+='wordlocation {0}'.format(tablenumber)      \n",
    "        clauselist+='{0}.wordid={1}'.format(tablenumber,wordid)\n",
    "        tablenumber+=1\n",
    "\n",
    "    # Create the query from the separate parts\n",
    "    fullquery='select {0} from {1} where {2}'.format(fieldlist,tablelist,clauselist)\n",
    "    print(fullquery)\n",
    "    cur=self.con.execute(fullquery)\n",
    "    rows=[row for row in cur]\n",
    "\n",
    "    return rows,wordids\n",
    "\n",
    "  def getscoredlist(self,rows,wordids):\n",
    "    totalscores=dict([(row[0],0) for row in rows])\n",
    "\n",
    "    # This is where we'll put our scoring functions\n",
    "    weights=[(1.0,self.locationscore(rows)), \n",
    "             (1.0,self.frequencyscore(rows)),\n",
    "             (1.0,self.pagerankscore(rows)),\n",
    "             (1.0,self.linktextscore(rows,wordids)),\n",
    "             (5.0,self.nnscore(rows,wordids))]\n",
    "    for (weight,scores) in weights:\n",
    "      for url in totalscores:\n",
    "        totalscores[url]+=weight*scores[url]\n",
    "\n",
    "    return totalscores\n",
    "\n",
    "  def geturlname(self,id):\n",
    "    return self.con.execute(\n",
    "    \"select url from urllist where rowid=%d\" % id).fetchone()[0]\n",
    "\n",
    "  def query(self,q):\n",
    "    rows,wordids=self.getmatchrows(q)\n",
    "    scores=self.getscoredlist(rows,wordids)\n",
    "    rankedscores=[(score,url) for (url,score) in scores.items()]\n",
    "    rankedscores.sort()\n",
    "    rankedscores.reverse()\n",
    "    for (score,urlid) in rankedscores[0:10]:\n",
    "      print('%f\\t%s' % (score,self.geturlname(urlid)))\n",
    "    return wordids,[r[1] for r in rankedscores[0:10]]\n",
    "\n",
    "  def normalizescores(self,scores,smallIsBetter=0):\n",
    "    vsmall=0.00001 # Avoid division by zero errors\n",
    "    if smallIsBetter:\n",
    "      minscore=min(scores.values())\n",
    "      return dict([(u,float(minscore)/max(vsmall,l)) for (u,l) in scores.items()])\n",
    "    else:\n",
    "      maxscore=max(scores.values())\n",
    "      if maxscore==0: maxscore=vsmall\n",
    "      return dict([(u,float(c)/maxscore) for (u,c) in scores.items()])\n",
    "\n",
    "  def frequencyscore(self,rows):\n",
    "    counts=dict([(row[0],0) for row in rows])\n",
    "    for row in rows: counts[row[0]]+=1\n",
    "    return self.normalizescores(counts)\n",
    "\n",
    "  def locationscore(self,rows):\n",
    "    locations=dict([(row[0],1000000) for row in rows])\n",
    "    for row in rows:\n",
    "      loc=sum(row[1:])\n",
    "      if loc<locations[row[0]]: locations[row[0]]=loc\n",
    "    \n",
    "    return self.normalizescores(locations,smallIsBetter=1)\n",
    "\n",
    "  def distancescore(self,rows):\n",
    "    # If there's only one word, everyone wins!\n",
    "    if len(rows[0])<=2: return dict([(row[0],1.0) for row in rows])\n",
    "\n",
    "    # Initialize the dictionary with large values\n",
    "    mindistance=dict([(row[0],1000000) for row in rows])\n",
    "\n",
    "    for row in rows:\n",
    "      dist=sum([abs(row[i]-row[i-1]) for i in range(2,len(row))])\n",
    "      if dist<mindistance[row[0]]: mindistance[row[0]]=dist\n",
    "    return self.normalizescores(mindistance,smallIsBetter=1)\n",
    "\n",
    "  def inboundlinkscore(self,rows):\n",
    "    uniqueurls=dict([(row[0],1) for row in rows])\n",
    "    inboundcount=dict([(u,self.con.execute('select count(*) from link where toid=%d' % u).fetchone()[0]) for u in uniqueurls])   \n",
    "    return self.normalizescores(inboundcount)\n",
    "\n",
    "  def linktextscore(self,rows,wordids):\n",
    "    linkscores=dict([(row[0],0) for row in rows])\n",
    "    for wordid in wordids:\n",
    "      cur=self.con.execute('select link.fromid,link.toid from linkwords,link where wordid=%d and linkwords.linkid=link.rowid' % wordid)\n",
    "      for (fromid,toid) in cur:\n",
    "        if toid in linkscores:\n",
    "          pr=self.con.execute('select score from pagerank where urlid=%d' % fromid).fetchone()[0]\n",
    "          linkscores[toid]+=pr\n",
    "    maxscore=max(linkscores.values())\n",
    "    normalizedscores=dict([(u,float(l)/maxscore) for (u,l) in linkscores.items()])\n",
    "    return normalizedscores\n",
    "\n",
    "  def pagerankscore(self,rows):\n",
    "    pageranks=dict([(row[0],self.con.execute('select score from pagerank where urlid=%d' % row[0]).fetchone()[0]) for row in rows])\n",
    "    maxrank=max(pageranks.values())\n",
    "    normalizedscores=dict([(u,float(l)/maxrank) for (u,l) in pageranks.items()])\n",
    "    return normalizedscores\n",
    "\n",
    "  def nnscore(self,rows,wordids):\n",
    "    # Get unique URL IDs as an ordered list\n",
    "    urlids=[urlid for urlid in dict([(row[0],1) for row in rows])]\n",
    "    nnres=mynet.getresult(wordids,urlids)\n",
    "    scores=dict([(urlids[i],nnres[i]) for i in range(len(urlids))])\n",
    "    return self.normalizescores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for text 'Programming in Scala'\n",
      "select w0.urlid from  where \n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "near \"where\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6a0d9e2257f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Programming in Scala'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Searching for text '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0msearcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-da818d79481d>\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, q)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwordids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetmatchrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetscoredlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwordids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mrankedscores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-da818d79481d>\u001b[0m in \u001b[0;36mgetmatchrows\u001b[1;34m(self, q)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mfullquery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'select {0} from {1} where {2}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfieldlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtablelist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclauselist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mcur\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: near \"where\": syntax error"
     ]
    }
   ],
   "source": [
    "#crawler = crawler(\"searchindex.db\")\n",
    "#crawler.createindextable(True)\n",
    "#crawler.createindextable()\n",
    "#crawler.crawl([\"https://en.wikipedia.org/wiki/Programming_language\", \"https://en.wikipedia.org/wiki/Functional_programming\"])\n",
    "searcher = Searcher('searchindex.db')\n",
    "#crawler.con.execute('create index wordurlidx_1 on wordlocation(urlid)')\n",
    "#Works badly for long queries, following for instance screws\n",
    "#results = searcher.getmatchrows(\"Functional programming with Scala and python\")\n",
    "\n",
    "#Following doesn't work too well and returns 123689 results\n",
    "q = 'Programming in Scala'\n",
    "print(\"Searching for text '%s'\" % q)\n",
    "searcher.query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urlid= 111\n",
    "wordid = 101\n",
    "i= 00\n",
    "t = \"insert into wordlocation(urlid,wordid,location) values (%d,%d,%d)\" % (urlid,wordid,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'insert into wordlocation(urlid,wordid,location) values (111,101,0)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
