{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "\"\"\"\n",
    "作者：王若愚\n",
    "工作：1.合并文章查询程序与获取文章内容程序\n",
    "     2.添加sql语句\n",
    "时间：2018/05/01 03:16\n",
    "\"\"\"\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import sched\n",
    "import queue\n",
    "#import Queue\n",
    "import requests\n",
    "import json\n",
    "import pymysql\n",
    "\n",
    "# 设定间隔时间 秒\n",
    "inc = 5\n",
    "\n",
    "class main(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url = 'https://www.ihuoqiu.com/MAPI/GetArticleListData'  # 首页网址url\n",
    "        #self.urlqueue = Queue.Queue() # python2 创建url的队列\n",
    "        self.urlqueue = queue.Queue()  #python3 创建url的队列\n",
    "        \n",
    "        # 模拟成浏览器\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.109 Safari/537.36',\n",
    "            'Referer': 'http://m.ihuoqiu.com/index',\n",
    "            'Origin': 'http://m.ihuoqiu.com',\n",
    "            'Connection':'keep-alive',\n",
    "            'Content-Type':'application/json;charset=utf-8',\n",
    "        }\n",
    "        \n",
    "        #设定schedule对象\n",
    "        self.schedule = sched.scheduler(time.time, time.sleep) \n",
    "\n",
    "        self.article = []  # 存放标题的变量\n",
    "        self.content = {}    # 存放文章内容\n",
    "    \n",
    "    def perform_command(self,inc):\n",
    "        self.schedule.enter(inc,0,self.perform_command,(inc,))\n",
    "        self.run_()\n",
    "    def timming_exe(self,inc = inc):\n",
    "        #enter 用来安排事件的发生\n",
    "        self.schedule.enter(inc,0,self.perform_command,(inc,))\n",
    "        self.schedule.run()\n",
    "        \n",
    "    def run_(self):\n",
    "        \n",
    "        # 该线程要执行的任务,即获得url队列,传入页面的url获得页面的代码\n",
    "        response = None\n",
    "        content = ''\n",
    "        \"\"\"\n",
    "        # 使用代理服务器访问网页\n",
    "        proxy_addr = {\n",
    "            \"http\": \"http://127.0.0.1:8080\",\n",
    "            \"https\": \"https://127.0.0.1:8080\"\n",
    "                      }\n",
    "        \"\"\"\n",
    "        # 构建请求的request\n",
    "        post_data = '''{\"Type\":1,\"PageIndex\":1,\"PageSize\":1000}'''\n",
    "        #r = requests.post(url=self.url, data=post_data, headers=self.headers, proxies=proxy_addr, verify=False)\n",
    "        r = requests.post(url=self.url, data=post_data, headers=self.headers)\n",
    "\n",
    "        # 从页面代码中获取title的链接\n",
    "        if r.status_code == 200:\n",
    "            # 获得页面代码\n",
    "            data = r.content\n",
    "            j_data = json.loads(data)\n",
    "            #print (j_data)\n",
    "\n",
    "            #将页面转化为json格式\n",
    "            if \"code\" in j_data.keys() and j_data.get(\"code\") == 200:\n",
    "                all_articles = j_data.get(\"data\", [])\n",
    "                all_articles_list = all_articles[2:-2].split(\"},{\")\n",
    "                ll = []\n",
    "                for one in all_articles_list:\n",
    "                    if one[0] == u\"{\" and one[-1] != u\"}\":\n",
    "                        one = one + u\"}\"\n",
    "                    elif one[0] != u\"{\" and one[-1] != u\"}\":\n",
    "                        one = u\"{\" + one + u\"}\"\n",
    "                    elif one[0] != u\"{\" and one[-1] == u\"}\":\n",
    "                        one = u\"{\" + one\n",
    "                    one = json.loads(one)\n",
    "                    ll.append(one)\n",
    "                    \n",
    "                for article_info in ll:\n",
    "                    ArticleInfo = article_info.get(\"ArticleInfo\", \"No finding the information\")\n",
    "                    #获取页面链接\n",
    "                    Url_data = article_info.get(\"data1\", None)\n",
    "                    #获取文章内容\n",
    "                    article_content = self.get_article_content(Url_data)\n",
    "                    Title  = ArticleInfo.get(\"Title\", \"No finding the content\")\n",
    "                    ImgUrl = ArticleInfo.get(\"ImgUrl\",\"No finding the img\")\n",
    "                    Source = ArticleInfo.get(\"Source\",\"No finding the source\")\n",
    "                    Author = ArticleInfo.get(\"Author\",\"No finding the author\")\n",
    "                    ShortDescription = ArticleInfo.get(\"ShortDescription\", \"No finding the description\")\n",
    "                    \n",
    "                    #将文章数据保存为json格式\n",
    "                    self.content = {\n",
    "                        \"Title\":Title,\n",
    "                        \"ImgUrl\":ImgUrl,\n",
    "                        \"Source\":Source,\n",
    "                        \"Author\":Author,\n",
    "                        \"Url_data\":Url_data,\n",
    "                        \"ShortDescription\":ShortDescription,\n",
    "                        \"Content\":article_content\n",
    "                    }\n",
    "                    self.article.append(self.content)\n",
    "                    \n",
    "            #show the list of article\n",
    "            print(\"The length is:\",len(self.article))\n",
    "            #展示采集数据\n",
    "            for i in self.article:\n",
    "                print(\"the article is:\",i)\n",
    "            \"\"\"\n",
    "            准备导入数据库\n",
    "            \"\"\"\n",
    "            self.import_to_db(self.article)\n",
    "        else:\n",
    "            print(u\"页面加载失败\")\n",
    "            time.sleep(100)\n",
    "            self.run()\n",
    "            return None\n",
    "    #将文章数据导入到mysql中\n",
    "    def import_to_db(self,art):\n",
    "        #创建 connection链接\n",
    "        connection = pymysql.connect(host='mysql-wang.cekkdnv29igt.us-west-2.rds.amazonaws.com',\n",
    "                       port=3306,user='scrapy',passwd='scrapy25806',charset='utf8')\n",
    "        try:\n",
    "            with connection.cursor() as cursor:\n",
    "                sql = \"CREATE database if not exists LWL\"\n",
    "                cursor.execute(sql)\n",
    "                cursor.execute(\"use LWL\")\n",
    "                #在该数据库中创建一个名为article的表\n",
    "                sql = \"CREATE TABLE if not exists article(Title VARCHAR(200),Author VARCHAR(20),Url_data VARCHAR(20),Content text)\"\n",
    "                cursor.execute(sql)\n",
    "                sql = \"INSERT INTO article(Title,Author,Content) VALUES(%s,%s,%s,%s)\"\n",
    "                try:\n",
    "                    #开始插入数据\n",
    "                    for db_art in art:\n",
    "                        cursor.execute(sql,(db_art.get(\"Title\", None),db_art.get(\"Author\", None),db_art.get(\"Url_data\",None),db_art.get(\"Content\",None)))\n",
    "                    connection.commit()\n",
    "                except:\n",
    "                    connection.rollback()\n",
    "                \"\"\"\n",
    "                增加一个去重功能\n",
    "                \"\"\"\n",
    "        finally:\n",
    "            connection.close()\n",
    "    def get_article_content(self,Url_data):\n",
    "        url = \"https://www.ihuoqiu.com/MAPI/GetArticleInfoData\"\n",
    "        headers = {\n",
    "            \"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\",\n",
    "            \"Connection\":\"keep-alive\",\n",
    "            \"Content-Type\":\"application/json;charset=UTF-8\",\n",
    "            \"Origin\":\"http://m.ihuoqiu.com\",\n",
    "            \"Referer\":\"http://m.ihuoqiu.com/article?id=\"+Url_data+\"&type=1\"\n",
    "        }\n",
    "        data = '''{Type: \"1\", data: \"%s\"}'''%Url_data\n",
    "        response =requests.post(url=url,data=data,headers=headers)\n",
    "        article1 = []\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            #将文章内容转化为正常格式\n",
    "            article_json = json.loads(response.content)\n",
    "            article_content = article_json.get(\"data\",[])\n",
    "            try:\n",
    "                art_content_list = article_content.split(',\"')\n",
    "                article = art_content_list[2].split(\"。<\")\n",
    "                for i in article:\n",
    "                    article1.append(BeautifulSoup(i,'lxml').get_text())\n",
    "                return article1\n",
    "            except:\n",
    "                print(\"The content is None\")\n",
    "                return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tit = main()\n",
    "try:    \n",
    "    tit.timming_exe(120)\n",
    "except ConnectionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping the 1 page...\n",
      "['https://www.7234.cn/news/53968', 'https://www.7234.cn/abroad/53969', 'https://www.7234.cn/news/53967', 'https://www.7234.cn/news/53963', 'https://www.7234.cn/news/53966', 'https://www.7234.cn/news/53964', 'https://www.7234.cn/news/53962', 'https://www.7234.cn/news/53970', 'https://www.7234.cn/news/53965', 'https://www.7234.cn/news/53961']\n",
      "the link is: https://www.7234.cn/news/53968\n",
      "the link is: https://www.7234.cn/abroad/53969\n",
      "the link is: https://www.7234.cn/news/53967\n",
      "the link is: https://www.7234.cn/news/53963\n",
      "the link is: https://www.7234.cn/news/53966\n",
      "the link is: https://www.7234.cn/news/53964\n",
      "the link is: https://www.7234.cn/news/53962\n",
      "the link is: https://www.7234.cn/news/53970\n",
      "the link is: https://www.7234.cn/news/53965\n",
      "the link is: https://www.7234.cn/news/53961\n",
      "scraping the 2 page...\n",
      "['https://www.7234.cn/news/53959', 'https://www.7234.cn/news/53958', 'https://www.7234.cn/news/53954', 'https://www.7234.cn/news/53950', 'https://www.7234.cn/news/53949', 'https://www.7234.cn/china/53945', 'https://www.7234.cn/news/53957', 'https://www.7234.cn/news/53956', 'https://www.7234.cn/news/53960', 'https://www.7234.cn/news/53955']\n",
      "the link is: https://www.7234.cn/news/53959\n",
      "the link is: https://www.7234.cn/news/53958\n",
      "the link is: https://www.7234.cn/news/53954\n",
      "the link is: https://www.7234.cn/news/53950\n",
      "the link is: https://www.7234.cn/news/53949\n",
      "the link is: https://www.7234.cn/china/53945\n",
      "the link is: https://www.7234.cn/news/53957\n",
      "the link is: https://www.7234.cn/news/53956\n",
      "the link is: https://www.7234.cn/news/53960\n",
      "the link is: https://www.7234.cn/news/53955\n"
     ]
    }
   ],
   "source": [
    "# encoding:utf-8\n",
    "'''\n",
    "作者：王若愚\n",
    "功能：\n",
    "     1.获取文章的页面链接\n",
    "     2.保存文章内容\n",
    "     3.保存文章图片\n",
    "注意事项：\n",
    "     1.爬取页面的内容可能隐藏与ajax之中\n",
    "     2.爬取开始之前需要查看network之中的response工具\n",
    "     3.创建的文件名字结尾不得有标点符号\n",
    "     4.创建的文件名字中不得出现冒号\n",
    "记录：\n",
    "     1.正则表达式在匹配时：(),[],{}的区别\n",
    "     2.部分未加<p></p>标签的段落，无法完整提取出来\n",
    "     3.不必加过多的try except语句\n",
    "     4.再次启动程序时，程序有时无法正常覆盖已有的爬取内容，\n",
    "       因此，若需重启程序时，建议手动更改原有保存路径或直接将\n",
    "       原纪录删除\n",
    "       \n",
    "时间：2018/05/01\n",
    "'''\n",
    "from urllib.request import urlretrieve\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class main(object):\n",
    "    def __init__(self):\n",
    "        self.baseurl = \"https://www.7234.cn\"        \n",
    "        #设置文件保存路径\n",
    "        self.path = \"H:\\Desktop\\scrapy\"\n",
    "        self.headers = {\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36',\n",
    "            'Connection':'keep-alive',\n",
    "            'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "            'Host': 'www.7234.cn'\n",
    "        }\n",
    "        \n",
    "    \n",
    "    #设置爬取的起始页面号码\n",
    "    def run(self,start_number=1,end_number=2):\n",
    "        for i in range(start_number,end_number+1,1):\n",
    "            post_data = {\"page\": i}#数字代表页数，通过改变数字爬取其他内容\n",
    "            print(\"scraping the %d page...\"%i)\n",
    "            url = \"https://www.7234.cn/fetch_articles/news?page=%s\"%(post_data[\"page\"])\n",
    "            #开始抓取每一个页面中出现的文章链接\n",
    "            self.get_article_link(url)\n",
    "    \n",
    "    def get_article_link(self,url):\n",
    "        artset = set()#保存文章链接的集合除重\n",
    "        #获取页面内容\n",
    "        res = requests.get(url)\n",
    "        if res.status_code == 200:\n",
    "            #将页面转化为json格式并获取html内容\n",
    "            data = json.loads(res.content)\n",
    "            html_json = data.get('html')\n",
    "            #print(html_json)\n",
    "            \n",
    "            #获取文章后缀匹配任意字符\n",
    "            artId = re.compile(\"href=(.*?) target\")\n",
    "            result1 = artId.findall(html_json)\n",
    "            #匹配类似于/news/53805\n",
    "            for i in result1:\n",
    "                number = re.compile(r'/\\w*/\\d+')\n",
    "                try:\n",
    "                    Id = re.search(number,i).group()\n",
    "                except AttributeError:\n",
    "                    Id = None\n",
    "                if Id != None:\n",
    "                    artset.add(self.baseurl+Id)\n",
    "            #保存文章链接\n",
    "            artlist = list(i for i in artset)\n",
    "            print(artlist)\n",
    "            self.get_article_content(artlist) \n",
    "            #print(\"You have gotten all the page...\")\n",
    "            \n",
    "            \n",
    "    def get_article_content(self,artlist):\n",
    "        #将工作路径赋值给path\n",
    "        path = self.path\n",
    "        #设定当前工作路径\n",
    "        if not os.path.exists(path):\n",
    "            try:\n",
    "                os.mkdir(path)\n",
    "            except:\n",
    "                print(\"This path is not valide\",path)\n",
    "        \n",
    "        for art in artlist:\n",
    "            print(\"the link is:\",art)\n",
    "            article_html = requests.get(url = art,headers = self.headers)\n",
    "            bs0bj = BeautifulSoup(article_html.text,'lxml')\n",
    "            #获取文章的标题\n",
    "            try:\n",
    "                title = bs0bj.find('div',class_=\"article-inner\").find(\"h1\").get_text()\n",
    "            except:\n",
    "                title = \"The article is None\"\n",
    "\n",
    "            #设定文章所在文件夹的名字\n",
    "            #注意文件夹名字不得用问号结尾且不得出现冒号\n",
    "            pattern = re.compile(r'[\\!,\\?,\\:,\\：,\\|]')\n",
    "            title = re.sub(pattern,\"_\",title)\n",
    "            path = path+\"\\\\\"+title\n",
    "            #创建文章文件夹用以保存文章内容的txt文件和图片\n",
    "            if not os.path.exists(path):\n",
    "                try:\n",
    "                    os.mkdir(path)\n",
    "                except:\n",
    "                    print(\"This path is not valide\",path)\n",
    "                \n",
    "            #文章内容链表\n",
    "            para_content = []\n",
    "            #文章图片链表\n",
    "            photo_link = []\n",
    "\n",
    "            #获取文章的内容\n",
    "            content = bs0bj.find(\"div\",{\"class\":\"a-content\"})\n",
    "            try:\n",
    "                paragraph = content.find_all(\"p\")\n",
    "                #分别对每个自然段进行分析\n",
    "                for p in paragraph:\n",
    "                    #获取文章中的图片\n",
    "                    para_content.append(p.get_text())\n",
    "                    try:\n",
    "                        ph = \"https:\"+p.find(\"img\")[\"src\"]\n",
    "                    except:\n",
    "                        ph = None\n",
    "                    if ph != None:\n",
    "                        photo_link.append(ph)\n",
    "            except:\n",
    "                paragraph = \"The content is none\"\n",
    "\n",
    "            #print(\"The title is:\",title)\n",
    "            #print(\"The content is:\",para_content)\n",
    "            try:\n",
    "                f = open(path+\"\\\\\"+title+\".txt\",\"w+\",encoding='utf8')\n",
    "                for i in para_content:\n",
    "                    f.write(i+'\\n')\n",
    "            except:\n",
    "                print(\"The file is not open\")\n",
    "            for i in range(len(photo_link)):\n",
    "                #try:\n",
    "                #print(\"The path of photo is: \"+path+\"\\\\\"+str(i)+\".jpg\")\n",
    "                urlretrieve (photo_link[i], path+\"\\\\\"+str(i)+\".jpg\")\n",
    "                #except:\n",
    "                    #print(\"The photo is not retrieved, \"+photo_link[i]+\" The link is: \"+art)\n",
    "                    #print(\"The path of photo is: \"+path+\"\\\\\"+str(i)+\".jpg\")\n",
    "            f.close()\n",
    "            path = self.path\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    title = main()\n",
    "try:    \n",
    "    title.run(1)\n",
    "except ConnectionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"H:\\Desktop\\wang_ruo_yu_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "photo = \"https://i1.7234.cn/system/redactor_assets/pictures_3/000/148/183/148183/2018/105ac507921e3ab9f80f50e9c70fac6a.jpeg\"\n",
    "urlretrieve(photo, \"H:Desktop\\wang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str = \"wang:ruo!yu?,\"\n",
    "pattern = re.compile(r'[\\!,\\?,\\:]')\n",
    "rep = re.sub(pattern,\"_\",str)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://i1.7234.cn/system/redactor_assets/pictures_3/000/148/104/148104/2018/8a895cc5326bfccc2f138b67a66fe316"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
