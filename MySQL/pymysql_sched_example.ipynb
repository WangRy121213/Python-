{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "def re_exe(cmd,inc = 10):\n",
    "    while True:\n",
    "        print(cmd)\n",
    "        time.sleep(inc)\n",
    "re_exe(\"fra\",12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "#coding=utf-8\n",
    "#这里需要引入三个模块\n",
    "import time, os, sched \n",
    "    \n",
    "# 第一个参数确定任务的时间，返回从某个特定的时间到现在经历的秒数 \n",
    "# 第二个参数以某种人为的方式衡量时间 \n",
    "schedule = sched.scheduler(time.time, time.sleep) \n",
    "    \n",
    "def perform_command(cmd, inc): \n",
    "    print(cmd) \n",
    "        \n",
    "def timming_exe(cmd, inc = 60): \n",
    "    # enter用来安排某事件的发生时间，从现在起第n秒开始启动 \n",
    "    schedule.enter(inc, 0, perform_command, (cmd, inc)) \n",
    "    # 持续运行，直到计划时间队列变成空为止 \n",
    "    schedule.run() \n",
    "        \n",
    "    \n",
    "print(\"show time after 10 seconds:\") \n",
    "timming_exe(\"This is the i time to run\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "#coding=utf-8\n",
    "\n",
    "#周期调用\n",
    "import time, os, sched \n",
    "    \n",
    "# 第一个参数确定任务的时间，返回从某个特定的时间到现在经历的秒数 \n",
    "# 第二个参数以某种人为的方式衡量时间 \n",
    "schedule = sched.scheduler(time.time, time.sleep) \n",
    "\n",
    "#该函数即为需要反复执行的函数\n",
    "#inc为间隔周期\n",
    "def perform_command(cmd, inc): \n",
    "    # 安排inc秒后再次运行自己，即周期运行 \n",
    "    schedule.enter(inc, 0, perform_command, (cmd, inc)) \n",
    "    print(\"perform_command: \",cmd) \n",
    "        \n",
    "def timming_exe(cmd, inc = 60): \n",
    "    # enter用来安排某事件的发生时间，从现在起第n秒开始启动 \n",
    "    schedule.enter(inc, 0, perform_command, (cmd, inc)) \n",
    "    # 持续运行，直到计划时间队列变成空为止 \n",
    "    schedule.run() \n",
    "        \n",
    "    \n",
    "print(\"show time after 10 seconds:\") \n",
    "timming_exe(\"This is the 0 time to run\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL\n",
    "\n",
    "   cmd窗口登陆数据库：\n",
    "        \n",
    "        mysql -u root -p\n",
    "   \n",
    "   创建，指定数据库存放数据：\n",
    "        \n",
    "       >CREATE DATABASE scraping   >USE scraping\n",
    "    \n",
    "   创建表 \n",
    "    \n",
    "       >CREATE TABLE pages(id BIGINT(7) NOT NULL AUTO_INCREMENT, title     VARCHAR(200),content VARCHAR(10000), created TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY(id))\n",
    "        \n",
    "   插入字段\n",
    "   \n",
    "       >INSERT INTO pages(title,content) VALUES (\"Test page title\", \"This is some test page content. It can be up to 10,000 characters long.\");\n",
    "   \n",
    "   数据选择和查询语句\n",
    "   \n",
    "       > SELECT * From pages WHERE id=2;\n",
    "       \n",
    "       > SELECT * FROM pages WHERE title LIKE \"%test%\"(title包含test的所有行)\n",
    "       \n",
    "       > SELECT id, title FROM pages WHERE content LIKE \"%page content%\"\n",
    "   \n",
    "   数据删除与更新\n",
    "    \n",
    "       > DELETE FROM pages WHERE id = 1;\n",
    "       \n",
    "       > UPDATE pages SET title=\"A new title\", content=\"Some new content\" WHERE id=2\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 与Python整合\n",
    "\n",
    "   1. 连接：\n",
    "      \n",
    "      'localhost' 服务器端口号\n",
    "      conn = pymysql.connect(host(str),port(int),user(str), passwd(str),db(str),charset(str)(连接编码))\n",
    "         \n",
    "   2. connection对象支持的方法：\n",
    "   \n",
    "      cursor():游标 commit():提交当前事物 rollback():回滚当前事务 close()\n",
    "   \n",
    "   3. cursor对象支持的方法:\n",
    "   \n",
    "      execute(op)      执行一个数据库的查询命令\n",
    "\n",
    "      fetchone()      取得结果集的下一行\n",
    "\n",
    "      fetchmany(size) 获取结果集的下几行\n",
    "\n",
    "      fetchall()      获取结果集中的所有行\n",
    "      \n",
    "      rowcount()      返回数据条数或影响行数\n",
    "      \n",
    "      close()         关闭游标对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 'A01', 'A01A', 'A01AA', 'A01AA01', 3400931911999, '0', 0, 20, 1, 11, 99, 26, '15,44', '51,48', 2014)\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "#创建一个连接\n",
    "conn = pymysql.connect(host='localhost',port=3306,user='root',passwd='25806')\n",
    "\n",
    "#创建一个游标\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"show databases\")\n",
    "\n",
    "#转换到eisti这个数据库\n",
    "cur.execute(\"USE eisti\")\n",
    "#执行一次选择\n",
    "cur.execute(\"SELECT * FROM open_medic WHERE AGE BETWEEN 12 and 60\")\n",
    "#取得结果集的下一行\n",
    "print(cur.fetchone())\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-96607b4fc7b7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-96607b4fc7b7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    CREATE TABLE `users` (\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "CREATE TABLE `users` (\n",
    "    `id` int(11) NOT NULL AUTO_INCREMENT,\n",
    "    `email` varchar(255) COLLATE utf8_bin NOT NULL,\n",
    "    `password` varchar(255) COLLATE utf8_bin NOT NULL,\n",
    "    PRIMARY KEY (`id`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin\n",
    "AUTO_INCREMENT=1 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#创建数据库并且设置编码utf-8多语言\n",
    "sql = \"CREATE database 'test_2'DEFAULT CHARACTER SET utf8 collate utf8_general_ci\"\n",
    "#创建表\n",
    "conn.execute(sql)\n",
    "#选择这个新的数据库\n",
    "conn.execute(\"use test_2\")\n",
    "#在该数据库中创建一个名为new_table的表\n",
    "sql = \"\"\"CREATE table article(Title VARCHAR(200),Author VARCHAR(20),Content VARCHAR(2000))\"\"\"\n",
    "conn.execute(sql)\n",
    "\n",
    "#插入操作\n",
    "sql = \"\"\"INSERT INTO article(Title,Author,Content)\n",
    "         VALUES('wang!success','wang','I am wangruoyu,finally I am successful')\"\"\"\"\n",
    "try:\n",
    "    #执行sql\n",
    "    cursor.execute(sql)\n",
    "    #提交到数据库执行\n",
    "    db.commit()\n",
    "except:\n",
    "    #如果发生错误\n",
    "    db.rollback()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#过滤重复记录 只显示一条记录\n",
    "Select * From HZT Where ID in (Select Max(ID) From HZT Group by Title)\n",
    "#查找全部重复记录\n",
    "SELECT * FROM t_info a WHERE ((SELECT COUNT(*) FROM t_info WHERE Title = a.Title) > 1) ORDER BY Title DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "connection = pymysql.connect(host='mysql-wang.cekkdnv29igt.us-west-2.rds.amazonaws.com',\n",
    "                       port=3306,user='scrapy',passwd='scrapy25806',charset='utf8')\n",
    "#cursor.execute(\"create table if not exists movie(name text, star text, quote text, info text)\")\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        #创建数据库并且设置编码utf-8多语言\n",
    "        sql = \"CREATE database if not exists test_2\"\n",
    "        cursor.execute(sql)  \n",
    "        #选择这个新的数据库\n",
    "        cursor.execute(\"use test_2\")\n",
    "        #在该数据库中创建一个名为new_table的表\n",
    "        sql = \"CREATE TABLE if not exists article(Title VARCHAR(200),Author VARCHAR(20),Content text)\"\n",
    "        cursor.execute(sql)\n",
    "        sql = \"INSERT INTO article(Title,Author,Content) VALUES(%s,%s,%s)\"\n",
    "        try:\n",
    "            #执行sql\n",
    "            cursor.execute(sql,('ruoyu!success','zhang','I am xiaoyue,finally I am successful'))\n",
    "            #提交到数据库执行\n",
    "            connection.commit()\n",
    "        except:\n",
    "            #如果发生错误\n",
    "            connection.rollback()\n",
    "        \n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "connection = pymysql.connect(host='mysql-wang.cekkdnv29igt.us-west-2.rds.amazonaws.com',\n",
    "                       port=3306,user='scrapy',passwd='scrapy25806',charset='utf8')\n",
    "\n",
    "#使用cursor()方法获取游标对象\n",
    "try:\n",
    "    with connection.cursor() as cursor:\n",
    "        sql = \"INSERT INTO `users` (`email`, `password`) VALUES (%s, %s)\"\n",
    "        cursor.execute(sql,('webmaster@python.org', 'very-secret'))\n",
    "    connection.commit()\n",
    "    with connection.cursor() as cursor:\n",
    "        sql = \"SELECT `id`, `password` FROM `users` WHERE `email`=%s\"\n",
    "        cursor.execute(sql, ('webmaster@python.org',))\n",
    "        result = cursor.fetchone()\n",
    "        print(result)\n",
    "finally:\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "'''\n",
    "Created on 2018年5月1日\n",
    "\n",
    "@author: liushouhua\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.7234.cn/news/53774\"\n",
    "res = requests.get(url)\n",
    "picture_pattern = re.compile('<p><img src=\"//(.*?)\"><',re.S)\n",
    "time_pattern = re.compile('</span><span>(.*?)</span>&nbsp',re.S)\n",
    "content_pattern = re.compile('</script>.*?</div>(.*?)</p>',re.S)\n",
    "if res.status_code == 200:\n",
    "    contentlist = []\n",
    "    data = res.content\n",
    "#     print data\n",
    "    time_item = re.findall(time_pattern,data)\n",
    "    ll=time_item[0].split(\"<span>\")\n",
    "    time_item=ll[-1]\n",
    "    \n",
    "    print(time_item)\n",
    "    picture_item = re.findall(picture_pattern,data)\n",
    "#     content_item = re.findall(content_pattern,data)\n",
    "#     print content_item\n",
    "#     for item in content_item:\n",
    "#         print item\n",
    "#         contentlist.append(BeautifulSoup(item,'lxml').get_text())\n",
    "#         print contentlist\n",
    "    \n",
    "    \n",
    "    content=BeautifulSoup(data,'lxml').get_text()\n",
    "#     print content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "'''\n",
    "作者：王若愚\n",
    "功能：\n",
    "     1.获取文章的页面链接\n",
    "     2.保存文章内容\n",
    "     3.保存文章图片\n",
    "时间：2018/05/01\n",
    "'''\n",
    "from urllib.request import urlretrieve\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class main(object):\n",
    "    def __init__(self):\n",
    "        self.baseurl = \"https://www.7234.cn\"        \n",
    "        self.artset = set()#保存文章链接的集合除重\n",
    "        #设置文件保存路径\n",
    "        self.path = \"H:\\Desktop\\scrapy\"\n",
    "    \n",
    "    #设置爬取的起始页面号码\n",
    "    def run(self,start_number=1,end_number=2):\n",
    "        for i in range(start_number,end_number+1,1):\n",
    "            post_data = {\"page\": i}#数字代表页数，通过改变数字爬取其他内容\n",
    "            url = \"https://www.7234.cn/fetch_articles/news?page=%s\"%(post_data[\"page\"])\n",
    "            #开始抓取每一个页面中出现的文章链接\n",
    "            self.get_article_link(url)\n",
    "    \n",
    "    def get_article_link(self,url):\n",
    "        #获取页面内容\n",
    "        res = requests.get(url)\n",
    "        if res.status_code == 200:\n",
    "            #将页面转化为json格式并获取html内容\n",
    "            data = json.loads(res.content)\n",
    "            html_json = data.get('html')\n",
    "            #print(html_json)\n",
    "            \n",
    "            #获取文章后缀匹配任意字符\n",
    "            artId = re.compile(\"href=(.*?) target\")\n",
    "            result1 = artId.findall(html_json)\n",
    "            #匹配类似于/news/53805\n",
    "            for i in result1:\n",
    "                number = re.compile(r'/\\w*/\\d+')\n",
    "                try:\n",
    "                    Id = re.search(number,i).group()\n",
    "                except AttributeError:\n",
    "                    Id = None\n",
    "                if Id != None:\n",
    "                    artset.add(self.baseurl+Id)\n",
    "            #保存文章链接\n",
    "            artlist = list(i for i in artset)\n",
    "            print(artlist)\n",
    "            self.get_article_content(artlist) \n",
    "    def get_article_content(self,art):\n",
    "        path = self.path\n",
    "        #设定当前工作路径\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        for art in artlist:\n",
    "            print(\"the link is:\",art)\n",
    "            article_html = requests.get(art)\n",
    "            bs0bj = BeautifulSoup(article_html.text,'lxml')\n",
    "            #获取文章的标题\n",
    "            try:\n",
    "                title = bs0bj.find('div',class_=\"article-inner\").find(\"h1\").get_text()\n",
    "            except:\n",
    "                title = \"The article is None\"\n",
    "            #设定文章所在文件夹的名字\n",
    "            path = path+\"\\\\\"+title\n",
    "            #创建文章文件夹用以保存文章内容的txt文件和图片\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "                \n",
    "            #文章内容链表\n",
    "            para_content = []\n",
    "            #文章图片链表\n",
    "            photo_link = []\n",
    "\n",
    "            #获取文章的内容\n",
    "            content = bs0bj.find(\"div\",{\"class\":\"a-content\"})\n",
    "            try:\n",
    "                paragraph = content.find_all(\"p\")\n",
    "                #分别对每个自然段进行分析\n",
    "                for p in paragraph:\n",
    "                    #获取文章中的图片\n",
    "                    para_content.append(p.get_text())\n",
    "                    try:\n",
    "                        ph = \"https:\"+p.find(\"img\")[\"src\"]\n",
    "                    except:\n",
    "                        ph = None\n",
    "                    if ph != None:\n",
    "                        photo_link.append(ph)\n",
    "            except:\n",
    "                paragraph = \"The content is none\"\n",
    "\n",
    "            print(\"The title is:\",title)\n",
    "            print(\"The content is:\",para_content)\n",
    "            try:\n",
    "                f = open(path+\"\\\\\"+title+\".txt\",\"w+\",encoding='utf8')\n",
    "                for i in para_content:\n",
    "                    f.write(i+'\\n')\n",
    "            except:\n",
    "                print(\"The file is not open\")\n",
    "            print(\"The link is:\",photo_link)\n",
    "            for i in range(len(photo_link)):\n",
    "                urlretrieve (photo_link[i], path+\"\\\\\"+str(i)+\".jpg\")\n",
    "            f.close()\n",
    "            path = self.path\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    title = main()\n",
    "try:    \n",
    "    title.run(1)\n",
    "except ConnectionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "art = artlist[0]\n",
    "article_html = requests.get(art)\n",
    "bs0bj = BeautifulSoup(article_html.text,'lxml')\n",
    "\n",
    "title = bs0bj.find('div',class_=\"article-inner\").find(\"h1\").get_text()\n",
    "print(\"The title is:\",title)\n",
    "paragraph = bs0bj.find(\"div\",{\"class\":\"a-content\"}).find_all(\"p\")\n",
    "for p in paragraph:\n",
    "    print(\"The content is:\",p.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"H:\\Desktop\\wang\"\n",
    "if os.path.isdir(path):\n",
    "    print(\"exit\")\n",
    "else:\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import re\n",
    "\n",
    "#设定当前工作目录\n",
    "path = \"H:\\Desktop\\scrapy\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "art = \"https://www.7234.cn/news/53905\"\n",
    "print(\"the link is:\",art)\n",
    "article_html = requests.get(art)\n",
    "bs0bj = BeautifulSoup(article_html.text,'lxml')\n",
    "\n",
    "#获取文章的标题\n",
    "try:\n",
    "    title = bs0bj.find('div',class_=\"article-inner\").find(\"h1\").get_text()\n",
    "except:\n",
    "    title = \"The article is None\"\n",
    "path = \"H:\\Desktop\\scrapy\"+\"\\\\\"+title\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "#文章内容链表\n",
    "para_content = []\n",
    "#文章图片链表\n",
    "photo_link = []\n",
    "\n",
    "#获取文章的内容\n",
    "content = bs0bj.find(\"div\",{\"class\":\"a-content\"})\n",
    "try:\n",
    "    paragraph = content.find_all(\"p\")\n",
    "    for p in paragraph:\n",
    "        #获取文章中的图片\n",
    "        para_content.append(p.get_text())\n",
    "        try:\n",
    "            ph = \"https:\"+p.find(\"img\")[\"src\"]\n",
    "        except:\n",
    "            ph = None\n",
    "        if ph != None:\n",
    "            photo_link.append(ph)\n",
    "except:\n",
    "    paragraph = \"The content is none\"\n",
    "\n",
    "print(\"The title is:\",title)\n",
    "print(\"The content is:\",para_content)\n",
    "try:\n",
    "    f = open(path+\"\\\\\"+title+\".txt\",\"w+\",encoding='utf8')\n",
    "    for i in para_content:\n",
    "        f.write(i+'\\n')\n",
    "except:\n",
    "    print(\"The file is not open\")\n",
    "print(\"The link is:\",photo_link)\n",
    "for i in range(len(photo_link)):\n",
    "    urlretrieve(photo_link[i], path+\"\\\\\"+str(i)+\".jpg\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Xiaoyue,loves,ruoyu\"\n",
    "strings = string.split(\",\")\n",
    "print(strings)\n",
    "print(\"Xiaoyu\"+\"\\n\"+\"loves\"+\"\\n\"+\"ruoyu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = content.split(\"</\")\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = content.get_text().split(\"\\n\")\n",
    "for i in range(len(test)):\n",
    "    print(i)\n",
    "    print(test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://www.pythonscraping.com\")\n",
    "bsObj = BeautifulSoup(html)\n",
    "imageLocation = bsObj.find(\"a\", {\"id\": \"logo\"}).find(\"img\")[\"src\"]\n",
    "urlretrieve (imageLocation, \"H:/Desktop/logo.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.ihuoqiu.com/MAPI/GetArticleInfoData\"\n",
    "url_data = \"x8s4uNphJVUkxLR7EjGwBw__2C__2C\"\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36\",\n",
    "    \"Connection\":\"keep-alive\",\n",
    "    \"Content-Type\":\"application/json;charset=UTF-8\",\n",
    "    \"Origin\":\"http://m.ihuoqiu.com\",\n",
    "    \"Referer\":\"http://m.ihuoqiu.com/article?id=x8s4uNphJVUkxLR7EjGwBw__2C__2C&type=1\"\n",
    "}\n",
    "data = '''{Type: \"1\", data: \"%s\"}'''%url_data\n",
    "response =requests.post(url=url,data=data,headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    article_json = json.loads(response.content)\n",
    "    article_content = article_json.get(\"data\",[])\n",
    "    art_content_list = article_content.split(',\"')\n",
    "    article = art_content_list[2].split(\"。<\")\n",
    "    article1 = []\n",
    "    \n",
    "    for i in article:\n",
    "        article1.append(BeautifulSoup(i,'lxml').get_text())\n",
    "    \n",
    "    for j in article1:\n",
    "        print(j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
